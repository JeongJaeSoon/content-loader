"""Demo executor for testing core functionality."""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Any, AsyncGenerator, Dict, List

from content_loader.core import (
    BaseExecutor,
    ContentType,
    DateRange,
    Document,
    DocumentMetadata,
    Settings,
    SourceType,
)
from content_loader.services import EmbeddingService, VectorStore
from content_loader.services.document_processor import DocumentProcessor

logger = logging.getLogger(__name__)


class DemoExecutor(BaseExecutor):
    """Demo executor for testing core functionality."""

    def __init__(self, config: dict):
        super().__init__(config)
        self.source_name = config.get("source_name", "demo")
        self.document_count = config.get("document_count", 5)

        # Initialize embedding pipeline
        settings = Settings()
        self.embedding_service = EmbeddingService()
        self.vector_store = VectorStore(qdrant_url=settings.qdrant_url)
        self.document_processor = DocumentProcessor(
            self.embedding_service, self.vector_store
        )

    async def fetch(self, date_range: DateRange) -> AsyncGenerator[Document, None]:
        """Generate demo documents with rich sample data."""
        logger.info(
            f"Demo executor ({self.source_name}): "
            f"Generating {self.document_count} documents"
        )

        # Sample content templates for different types
        sample_contents: List[Dict[str, Any]] = [
            {
                "title": "Team Meeting Notes",
                "text": "Discussed project milestones and upcoming deadlines. "
                "Key decisions made regarding the new feature implementation. "
                "Action items assigned to team members.",
                "content_type": ContentType.DOCUMENTATION,
            },
            {
                "title": "Technical Discussion",
                "text": "Explored different approaches to implementing the new API. "
                "Considered performance implications and scalability concerns. "
                "Decided on microservices architecture.",
                "content_type": ContentType.CONVERSATION,
            },
            {
                "title": "Code Review Feedback",
                "text": "Reviewed the latest pull request for the authentication module. "
                "Found several optimization opportunities and suggested "
                "improvements to error handling.",
                "content_type": ContentType.SOURCE_CODE,
            },
            {
                "title": "Product Roadmap Update",
                "text": "Updated the product roadmap with new feature priorities. "
                "Adjusted timelines based on customer feedback and market research "
                "findings.",
                "content_type": ContentType.DOCUMENTATION,
            },
            {
                "title": "Bug Investigation",
                "text": "Investigated the reported issue with user authentication. "
                "Found the root cause in the token validation logic. "
                "Proposed fix and testing approach.",
                "content_type": ContentType.CONVERSATION,
            },
        ]

        for i in range(self.document_count):
            await asyncio.sleep(0.1)  # Simulate async work

            # Use sample content or generate generic content
            if i < len(sample_contents):
                sample = sample_contents[i]
                title = f"{sample['title']} - {self.source_name}"
                text = f"{sample['text']} (Generated by {self.source_name} demo loader)"
                content_type = sample["content_type"]
            else:
                title = f"Demo Document {i} from {self.source_name}"
                text = (
                    f"This is demo document content {i} from "
                    f"{self.source_name}. " * (i + 1)
                )
                content_type = ContentType.DOCUMENTATION

            metadata = DocumentMetadata(
                source_type=SourceType.SLACK,
                source_id=f"demo_{self.source_name}_{i}",
                source_url=f"https://demo.example.com/{self.source_name}/{i}",
                content_type=content_type,
                created_at=datetime.now() - timedelta(days=i),
                updated_at=datetime.now() - timedelta(hours=i),
            )

            document = Document(
                id=f"demo_{self.source_name}_{i}",
                title=title,
                text=text,
                metadata=metadata,
                url=f"https://demo.example.com/{self.source_name}/{i}",
                created_at=metadata.created_at,
                updated_at=metadata.updated_at,
            )

            logger.info(f"Generated document: {document.id}")
            yield document

    async def process_and_store_documents(self) -> None:
        """Process documents through the embedding pipeline."""
        logger.info("Starting document processing pipeline")

        # Generate documents
        date_range = DateRange(start=datetime.now() - timedelta(days=30))
        documents = self.fetch(date_range)

        # Process through embedding pipeline
        await self.document_processor.process_documents(documents)

        # Display collection info
        info = await self.vector_store.get_collection_info()
        logger.info(f"Vector store info: {info}")

    async def search_demo(self, query: str = "team meeting") -> None:
        """Demonstrate search functionality."""
        logger.info(f"Searching for: '{query}'")

        results = await self.document_processor.search_documents(query, limit=5)

        logger.info(f"Found {len(results)} results:")
        for i, result in enumerate(results, 1):
            logger.info(f"  {i}. Score: {result['score']:.3f}")
            logger.info(f"     Title: {result['payload'].get('title', 'N/A')}")
            logger.info(f"     Text: {result['payload'].get('text', 'N/A')[:100]}...")

    async def cleanup(self) -> None:
        """Clean up resources."""
        await self.vector_store.close()
