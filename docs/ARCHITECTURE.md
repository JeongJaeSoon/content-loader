# Content Loader ì•„í‚¤í…ì²˜ ì„¤ê³„

## ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜

Content LoaderëŠ” **ê³„ì¸µ ë¶„ë¦¬ ì•„í‚¤í…ì²˜**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

```
content-loader/
â”œâ”€â”€ main.py                    # CLI ì§„ì…ì 
â”œâ”€â”€ executor.py               # í†µí•© ì‹¤í–‰ê¸°
â”œâ”€â”€ settings.py               # ì „ì—­ ì„¤ì • ê´€ë¦¬
â”œâ”€â”€ core/                     # ê³µí†µ ê¸°ëŠ¥ (ê¸°ë°˜ ë ˆì´ì–´)
â””â”€â”€ loaders/                  # êµ¬í˜„ ë ˆì´ì–´
    â”œâ”€â”€ slack/
    â”œâ”€â”€ confluence/
    â””â”€â”€ github/
```

## ğŸ¯ í•µì‹¬ ì„¤ê³„ ì›ì¹™

### 1. ê³„ì¸µ ë¶„ë¦¬ + ìŠ¤íŠ¸ë¦¬ë° (Layered + Streaming)

- **core/**: ê³µí†µ ê¸°ë°˜ ê¸°ëŠ¥ (BaseExecutor, Models, Utils)
- **loaders/**: êµ¬ì²´ì ì¸ ë°ì´í„° ì†ŒìŠ¤ë³„ êµ¬í˜„ì²´
- **ì˜ì¡´ì„± ë°©í–¥**: `loaders/` â†’ `core/` (ë‹¨ë°©í–¥)
- **ìŠ¤íŠ¸ë¦¬ë°**: AsyncGeneratorë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬

### 2. ë…ë¦½ì„± + ê²¬ê³ ì„± (Independence + Resilience)

- ê° loaderëŠ” ì™„ì „íˆ ë…ë¦½ì ìœ¼ë¡œ ê°œë°œ/ë°°í¬ ê°€ëŠ¥
- ìƒˆë¡œìš´ loader ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš”
- ì„¤ì •ê³¼ ì½”ë“œê°€ ë™ì¼í•œ ë””ë ‰í† ë¦¬ì— ìœ„ì¹˜
- **í†µí•© ì¬ì‹œë„ ë¡œì§**ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ëŒ€ì‘

### 3. í™•ì¥ì„± + ì‹¬í”Œí•¨ (Extensibility + Simplicity)

- ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤(`BaseExecutor`) êµ¬í˜„ìœ¼ë¡œ ì¼ê´€ì„± ìœ ì§€
- í”ŒëŸ¬ê·¸ì¸ ë°©ì‹ìœ¼ë¡œ ìƒˆ loader ì¶”ê°€ ê°€ëŠ¥
- **ë³µì¡í•œ íŒ¨í„´ ë°°ì œ**, í•µì‹¬ ê¸°ëŠ¥ë§Œ êµ¬í˜„

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### **1. Core Layer (ê³µí†µ ê¸°ëŠ¥)**

```python
# core/base.py - í†µí•© ì‹¤í–‰ê¸° íŒ¨í„´
@dataclass
class DateRange:
    start: Optional[datetime] = None
    end: Optional[datetime] = None

    def includes(self, target_date: datetime) -> bool:
        if self.start and target_date < self.start:
            return False
        if self.end and target_date > self.end:
            return False
        return True

class BaseExecutor(ABC):
    """í†µí•© ì‹¤í–‰ê¸° - ìŠ¤íŠ¸ë¦¬ë° + ê²¬ê³ ì„±"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.retry_handler = SimpleRetryHandler()

    @abstractmethod
    async def fetch(self, date_range: DateRange) -> AsyncGenerator[Document, None]:
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¬¸ì„œ ë¡œë“œ"""
        pass

    async def execute(self, date_range: Optional[DateRange] = None) -> AsyncGenerator[Document, None]:
        """ì‹¤í–‰ + ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬"""
        if not date_range:
            date_range = DateRange()

        async for document in self.retry_handler.execute_with_retry(
            lambda: self.fetch(date_range)
        ):
            yield document

# core/models.py - ê³µí†µ ë°ì´í„° ëª¨ë¸
@dataclass
class Document:
    id: str
    title: str
    text: str
    metadata: Dict[str, Any]
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

# core/utils.py - ì‹¬í”Œí•œ ìœ í‹¸ë¦¬í‹°
class SimpleRetryHandler:
    """ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë¡œì§"""

    async def execute_with_retry(self, func_generator):
        for attempt in range(3):
            try:
                async for item in func_generator():
                    yield item
                break
            except (ConnectionError, TimeoutError) as e:
                if attempt < 2:
                    await asyncio.sleep(2 ** attempt)
                else:
                    raise

class SimpleMemoryManager:
    """ë°°ì¹˜ ë‹¨ìœ„ ë©”ëª¨ë¦¬ ê´€ë¦¬"""

    async def process_batch(self, documents_stream, batch_size=20):
        batch = []
        async for doc in documents_stream:
            batch.append(doc)
            if len(batch) >= batch_size:
                yield batch
                batch = []
        if batch:
            yield batch
```

### **2. Execution Layer (ì‹¤í–‰ ê³„ì¸µ) - ìŠ¤íŠ¸ë¦¬ë° ê¸°ë°˜**

```python
# executor.py - í†µí•© ì‹¤í–‰ê¸° (ìŠ¤íŠ¸ë¦¬ë°)
class LoaderExecutor:
    def __init__(self, settings: GlobalSettings):
        self.executors = {
            "slack": SlackExecutor(settings.slack),
            "github": GitHubExecutor(settings.github),
            "confluence": ConfluenceExecutor(settings.confluence)
        }
        self.memory_manager = SimpleMemoryManager()

    async def run_single_loader(self, loader_type: str, date_range: DateRange = None):
        """íŠ¹ì • loader ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰"""
        executor = self.executors[loader_type]

        # ìŠ¤íŠ¸ë¦¬ë° + ë°°ì¹˜ ì²˜ë¦¬
        async for batch in self.memory_manager.process_batch(
            executor.execute(date_range)
        ):
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë²¡í„°DB ì €ì¥
            # ë¬¸ì„œ ì²˜ë¦¬ + ì„ë² ë”© + ë²¡í„° ì €ì¥
            await self._process_and_store_batch(batch)

    async def run_all_loaders(self, date_range: DateRange = None):
        """ëª¨ë“  loader ë³‘ë ¬ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë°)"""
        tasks = []
        for loader_type in self.executors.keys():
            tasks.append(self.run_single_loader(loader_type, date_range))

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _process_and_store_batch(self, documents: List[Document]):
        """ë¬¸ì„œ ë°°ì¹˜ ì²˜ë¦¬ + ë²¡í„° ì €ì¥"""
        document_processor = DocumentProcessor()
        embedding_service = EmbeddingService()

        for document in documents:
            # 1. ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ + ì½˜í…ì¸ ë³„ ìš”ì•½
            processed_chunks = await document_processor.process_document(document)

            # 2. ì„ë² ë”© + Qdrant ì €ì¥
            await embedding_service.embed_and_store(processed_chunks, "content_vectors")
```

### **3. Configuration Layer (ì„¤ì • ê³„ì¸µ)**

```python
# settings.py - ì„¤ì • ê´€ë¦¬
class LoaderConfigManager:
    def load_loader_config(self, loader_name: str) -> dict:
        """loaderë³„ ì„¤ì • ë¡œë“œ"""
        config_dir = Path(f"loaders/{loader_name}/config")
        return self._load_with_env_override(config_dir)

    def load_loader_sources(self, loader_name: str) -> dict:
        """loaderë³„ ì†ŒìŠ¤ ì„¤ì • ë¡œë“œ"""
        # Slack: channels.yaml, GitHub: repositories.yaml, etc.
        pass
```

## ğŸ”„ ë°ì´í„° í”Œë¡œìš°

```mermaid
graph TD
    A[main.py] --> B[LoaderExecutor]
    B --> C[SlackLoader]
    B --> D[GitHubLoader]
    B --> E[ConfluenceLoader]

    C --> F[SlackClient]
    D --> G[GitHubClient]
    E --> H[ConfluenceClient]

    F --> I[Document Processing]
    G --> I
    H --> I

    I --> J[DocumentProcessor]
    J --> K[Smart Chunking]
    K --> L[Content-aware Summarization]
    L --> M[EmbeddingService]
    M --> N[Qdrant Vector DB]
```

## ğŸ›¡ï¸ ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬

### 1. ì‹¬í”Œí•œ ì¬ì‹œë„ ë¡œì§

```python
class SimpleRetryHandler:
    """ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„ ë¡œì§"""

    async def execute_with_retry(self, func_generator, max_retries=3):
        for attempt in range(max_retries):
            try:
                async for item in func_generator():
                    yield item
                break
            except (ConnectionError, TimeoutError, aiohttp.ClientError) as e:
                if attempt < max_retries - 1:
                    backoff_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                    await asyncio.sleep(backoff_time)
                else:
                    raise
```

### 2. ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ë³µêµ¬ (ì¦ë¶„ ì—…ë°ì´íŠ¸)

```python
class CheckpointManager:
    """DateRange ê¸°ë°˜ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬"""

    async def save_progress(self, source_key: str, last_modified: datetime):
        """ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ì €ì¥"""
        await self.cache_client.set(
            f"checkpoint:{source_key}",
            last_modified.isoformat(),
            expire=86400*30  # 30ì¼ ë³´ê´€
        )

    async def get_last_checkpoint(self, source_key: str) -> Optional[datetime]:
        """ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ë¡œ DateRange êµ¬ì„±"""
        checkpoint = await self.cache_client.get(f"checkpoint:{source_key}")
        return datetime.fromisoformat(checkpoint) if checkpoint else None
```

## ğŸš€ ì„±ëŠ¥ ìµœì í™”

### 1. ìŠ¤íŠ¸ë¦¬ë° ë©”ëª¨ë¦¬ ê´€ë¦¬

```python
class SimpleMemoryManager:
    """ìŠ¤íŠ¸ë¦¬ë° ê¸°ë°˜ ë°°ì¹˜ ì²˜ë¦¬"""

    async def process_batch(self, documents_stream, batch_size=20):
        """ìŠ¤íŠ¸ë¦¬ë° + ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±"""
        batch = []
        async for doc in documents_stream:
            batch.append(doc)

            if len(batch) >= batch_size:
                yield batch
                batch = []
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                import gc; gc.collect()

        if batch:  # ë§ˆì§€ë§‰ ë°°ì¹˜
            yield batch
```

### 2. í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬

```python
class BaseExecutor:
    """ì»¤ì„œ ê¸°ë°˜ í˜ì´ì§€ë„¤ì´ì…˜"""

    async def _paginate_fetch(self, initial_params: dict):
        """ì»¤ì„œ ê¸°ë°˜ìœ¼ë¡œ í˜ì´ì§€ë³„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        cursor = None

        while True:
            params = {**initial_params}
            if cursor:
                params['cursor'] = cursor

            response = await self._make_request(params)

            for item in response.get('items', []):
                yield item

            # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
            if not response.get('has_next_page'):
                break
            cursor = response.get('next_cursor')
```

### 3. ì—°ê²° í’€ ê´€ë¦¬

```python
class SimpleClient:
    """ì¬ì‹œë„ ë¡œì§ + ì—°ê²° ê´€ë¦¬"""

    def __init__(self, settings):
        self.semaphore = asyncio.Semaphore(settings.max_concurrent)
        self.retry_handler = SimpleRetryHandler()

    async def make_request(self, url: str, **kwargs):
        async with self.semaphore:
            # ì¬ì‹œë„ ë¡œì§ ì ìš©
            async for response in self.retry_handler.execute_with_retry(
                lambda: self._single_request(url, **kwargs)
            ):
                return response
```

## ğŸ§  ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥

### 1. Content-Aware Processing

```python
class DocumentProcessor:
    """ì½˜í…ì¸  íƒ€ì…ë³„ ìµœì í™”ëœ ë¬¸ì„œ ì²˜ë¦¬"""

    def __init__(self):
        self.content_detector = ContentTypeDetector()
        self.summarization_strategies = {
            "source_code": {"should_summarize": False, "chunk_strategy": "function_based"},
            "documentation": {"should_summarize": True, "chunk_strategy": "semantic"},
            "conversation": {"should_summarize": True, "chunk_strategy": "thread_based"},
            "mixed_content": {"should_summarize": True, "chunk_strategy": "adaptive"}
        }

    async def process_document(self, document: Document) -> List[ProcessedChunk]:
        """ë¬¸ì„œ íƒ€ì…ì— ë”°ë¥¸ ì°¨ë³„í™”ëœ ì²˜ë¦¬"""

        # 1. ì½˜í…ì¸  íƒ€ì… ìë™ ê°ì§€
        content_type = self.content_detector.detect_content_type(document)
        strategy = self.summarization_strategies[content_type]

        # 2. íƒ€ì…ë³„ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
        chunks = await self._smart_chunk(document, strategy["chunk_strategy"])

        processed_chunks = []
        for chunk in chunks:
            # 3. ì›ë³¸ ì²­í¬ í•­ìƒ ìƒì„± (ë¼ì¸ ì •ë³´ í¬í•¨)
            processed_chunks.append(self._create_chunk(chunk, "original", content_type))

            # 4. ìš”ì•½ ì²­í¬ ìƒì„± (í•„ìš”ì‹œë§Œ)
            if strategy["should_summarize"] and len(chunk.text) > 500:
                summary = await self._summarize_chunk(chunk, content_type)
                processed_chunks.append(self._create_chunk(summary, "summary", content_type))

        return processed_chunks

class ContentTypeDetector:
    """ë©”íƒ€ë°ì´í„° + ë‚´ìš© ë¶„ì„ìœ¼ë¡œ ì½˜í…ì¸  íƒ€ì… ê°ì§€"""

    def detect_content_type(self, document: Document) -> str:
        # GitHub ì†ŒìŠ¤ì½”ë“œ íŒŒì¼ ì²´í¬
        if document.metadata.get('source_type') == 'github':
            file_path = document.metadata.get('file_path', '')
            if file_path.endswith(('.py', '.js', '.ts', '.java', '.cpp', '.go')):
                return "source_code"
            elif file_path.endswith(('.md', '.rst', '.txt')):
                return "documentation"

        # Slackì€ ëŒ€í™”í˜•
        elif document.metadata.get('source_type') == 'slack':
            return "conversation"

        # ConfluenceëŠ” ë¬¸ì„œ
        elif document.metadata.get('source_type') == 'confluence':
            return "documentation"

        # ë‚´ìš© ë¶„ì„ìœ¼ë¡œ ë¶„ë¥˜
        return self._analyze_content(document.text)

class SmartCodeParser:
    """Tree-sitter ê¸°ë°˜ AST ì½”ë“œ ì²­í‚¹"""

    def __init__(self):
        self.MAX_BLOCK_CHARS = 1024
        self.MIN_BLOCK_CHARS = 200
        self.TOLERANCE_FACTOR = 1.5

    async def chunk_code_file(self, document: Document) -> List[CodeChunk]:
        """AST ê¸°ë°˜ ê³„ì¸µì  ì½”ë“œ ì²­í‚¹"""
        file_path = document.metadata.get('file_path', '')
        language = self._detect_language(file_path)

        # 1. Tree-sitterë¡œ AST ìƒì„±
        tree = self._parse_with_tree_sitter(document.text, language)

        # 2. ê³„ì¸µì  ë…¸ë“œ ì¶”ì¶œ
        code_blocks = self._extract_code_blocks(tree.root_node, document.text)

        # 3. í¬ê¸° ê¸°ë°˜ ìë™ ì²­í‚¹
        chunks = []
        for block in code_blocks:
            if len(block.text) > self.MAX_BLOCK_CHARS * self.TOLERANCE_FACTOR:
                # í° ë¸”ë¡ì€ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• 
                sub_chunks = await self._chunk_large_block(block)
                chunks.extend(sub_chunks)
            else:
                chunks.append(block)

        return chunks

    async def _chunk_large_block(self, block: CodeBlock) -> List[CodeChunk]:
        """í° ì½”ë“œ ë¸”ë¡ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """

        # 1. ìì‹ ë…¸ë“œê°€ ìˆìœ¼ë©´ ìì‹ ë‹¨ìœ„ë¡œ ë¶„í• 
        if block.children:
            chunks = []
            for child in block.children:
                if len(child.text) >= self.MIN_BLOCK_CHARS:
                    chunks.append(CodeChunk(
                        text=child.text,
                        start_line=child.start_line,
                        end_line=child.end_line,
                        node_type=child.node_type,
                        hash=self._generate_hash(child.text)
                    ))
            return chunks

        # 2. ë¦¬í”„ ë…¸ë“œëŠ” ë¼ì¸ ê¸°ë°˜ ë¶„í• 
        return self._chunk_by_lines(block)

    def _chunk_by_lines(self, block: CodeBlock) -> List[CodeChunk]:
        """ë¼ì¸ ë‹¨ìœ„ ì²­í‚¹ (ì˜ë¯¸ ë³´ì¡´)"""
        lines = block.text.split('\n')
        chunks = []
        current_chunk = []
        current_size = 0
        start_line = block.start_line

        for i, line in enumerate(lines):
            current_chunk.append(line)
            current_size += len(line)

            # ì²­í¬ í¬ê¸° ì²´í¬
            if (current_size >= self.MIN_BLOCK_CHARS and
                current_size <= self.MAX_BLOCK_CHARS):

                # ì˜ë¯¸ìˆëŠ” ëì ì—ì„œ ë¶„í•  (í•¨ìˆ˜ ë, í´ë˜ìŠ¤ ë ë“±)
                if self._is_good_break_point(line):
                    chunks.append(CodeChunk(
                        text='\n'.join(current_chunk),
                        start_line=start_line,
                        end_line=start_line + len(current_chunk) - 1,
                        node_type=block.node_type,
                        hash=self._generate_hash('\n'.join(current_chunk))
                    ))
                    current_chunk = []
                    current_size = 0
                    start_line = start_line + i + 1

        # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if current_chunk and current_size >= self.MIN_BLOCK_CHARS:
            chunks.append(CodeChunk(
                text='\n'.join(current_chunk),
                start_line=start_line,
                end_line=start_line + len(current_chunk) - 1,
                node_type=block.node_type,
                hash=self._generate_hash('\n'.join(current_chunk))
            ))

        return chunks
```

### 2. ì„ë² ë”© ë° ë²¡í„° ì €ì¥

```python
class EmbeddingService:
    """ë°°ì¹˜ ê¸°ë°˜ ì„ë² ë”© + Qdrant ì €ì¥"""

    def __init__(self):
        self.model_name = "text-embedding-3-small"
        self.qdrant_client = QdrantClient()

    async def embed_and_store(self, chunks: List[ProcessedChunk], collection: str):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© + ì €ì¥"""
        batch_size = 50

        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]

            # 1. ì„ë² ë”© ìƒì„±
            texts = [chunk.text for chunk in batch]
            embeddings = await self._generate_embeddings(texts)

            # 2. Qdrant í¬ì¸íŠ¸ êµ¬ì„±
            points = [
                PointStruct(
                    id=chunk.id,
                    vector=embedding,
                    payload={
                        "text": chunk.text,
                        "content_type": chunk.metadata.get("content_type"),
                        "chunk_type": chunk.metadata.get("chunk_type"),  # original/summary
                        "source_type": chunk.metadata.get("source_type"),
                        "start_line": chunk.metadata.get("start_line"),  # ì½”ë“œ ë¼ì¸ ì •ë³´
                        "end_line": chunk.metadata.get("end_line"),
                        "node_type": chunk.metadata.get("node_type"),    # function, class ë“±
                        "code_hash": chunk.metadata.get("hash"),         # ì¤‘ë³µ ë°©ì§€
                        **chunk.metadata
                    }
                )
                for chunk, embedding in zip(batch, embeddings)
            ]

            # 3. ë²¡í„° ì €ì¥
            await self.qdrant_client.upsert(collection_name=collection, points=points)
```

### 3. ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ê¸°ì¤€

```python
class SmartSummarizationRules:
    """ì‹¤ìš©ì ì¸ ìš”ì•½ ê¸°ì¤€"""

    @staticmethod
    def should_summarize(chunk: Chunk, content_type: str) -> bool:
        """ì²­í¬ë³„ ìš”ì•½ í•„ìš” ì—¬ë¶€ íŒë‹¨"""

        # 1. ì†ŒìŠ¤ì½”ë“œë„ ê¸¸ë©´ ìš”ì•½ (ë‹¨, ì‹ ì¤‘í•˜ê²Œ)
        if content_type == "source_code":
            # ë§¤ìš° ê¸´ ì½”ë“œë§Œ ìš”ì•½ (ì£¼ì„/docstring ìœ„ì£¼)
            return text_length >= 2000

        # 2. í¬ê¸° ê¸°ì¤€ (í•µì‹¬!)
        text_length = len(chunk.text)
        if text_length < 300:  # ë„ˆë¬´ ì§§ìœ¼ë©´ ìš”ì•½ ë¶ˆí•„ìš”
            return False
        if text_length < 800:  # ì ë‹¹í•œ í¬ê¸°ë©´ ì„ íƒì 
            return content_type in ["conversation"]  # ëŒ€í™”ë§Œ ìš”ì•½
        if text_length >= 1500:  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë¬´ì¡°ê±´ ìš”ì•½
            return True

        # 3. ì½˜í…ì¸  íƒ€ì…ë³„ ì¤‘ê°„ í¬ê¸° ì²˜ë¦¬
        return {
            "documentation": text_length >= 600,  # ë¬¸ì„œëŠ” ì¡°ê¸ˆ ë” ê´€ëŒ€
            "conversation": text_length >= 400,   # ëŒ€í™”ëŠ” ë¹¨ë¦¬ ìš”ì•½
            "mixed_content": text_length >= 500   # MixedëŠ” ì¤‘ê°„
        }.get(content_type, False)

# ì‹¤ì œ ìš”ì•½ ê¸°ì¤€ í…Œì´ë¸”
SUMMARIZATION_THRESHOLDS = {
    "source_code": {"enabled": True, "min_length": 2000, "prompt": "ì½”ë“œì˜ ê¸°ëŠ¥ê³¼ ëª©ì ì„ ê°„ë‹¨íˆ ì„¤ëª…:"},
    "documentation": {"enabled": True, "min_length": 600},
    "conversation": {"enabled": True, "min_length": 400},
    "mixed_content": {"enabled": True, "min_length": 500}
}
```

### 4. ìš”ì•½ ê¸°ì¤€ ìš”ì•½

| ì½˜í…ì¸  íƒ€ì… | ê¸°ë³¸ ì •ì±… | ìµœì†Œ ê¸¸ì´ | ë¬´ì¡°ê±´ ìš”ì•½ | ìš”ì•½ ë°©ì‹ |
|------------|----------|----------|------------|----------|
| **ì†ŒìŠ¤ì½”ë“œ** | ğŸŒ³ AST ê¸°ë°˜ ì²­í‚¹ | 200-1024ì ë‹¨ìœ„ | 2000ì ì´ìƒ | Tree-sitterë¡œ ì˜ë¯¸ ë‹¨ìœ„ ë¶„í•  |
| **ë¬¸ì„œ** | ğŸ“ í¬ê¸° ê¸°ì¤€ | 600ì ì´ìƒ | 1500ì ì´ìƒ | í•µì‹¬ ë‚´ìš© ìš”ì•½ |
| **ëŒ€í™”** | ğŸ“ í¬ê¸° ê¸°ì¤€ | 400ì ì´ìƒ | 1500ì ì´ìƒ | ê²°ë¡ /ì•¡ì…˜ ì•„ì´í…œ |
| **Mixed** | ğŸ“ í¬ê¸° ê¸°ì¤€ | 500ì ì´ìƒ | 1500ì ì´ìƒ | ì„ íƒì  ìš”ì•½ |

**ğŸ’¡ ìŠ¤ë§ˆíŠ¸ ì½”ë“œ ì²­í‚¹ ì•„ì´ë””ì–´**:
- **AST ê¸°ë°˜ ë¶„í• **: Tree-sitterë¡œ í•¨ìˆ˜/í´ë˜ìŠ¤ ë‹¨ìœ„ ì˜ë¯¸ ë³´ì¡´
- **ê³„ì¸µì  ì²­í‚¹**: í° ë¸”ë¡ â†’ ìì‹ ë…¸ë“œ â†’ ë¼ì¸ ë‹¨ìœ„ ìˆœì°¨ ë¶„í• 
- **ë¼ì¸ ì •ë³´ ë³´ì¡´**: start_line, end_lineìœ¼ë¡œ ì •í™•í•œ ìœ„ì¹˜ ì¶”ì 
- **ì¤‘ë³µ ë°©ì§€**: ì½”ë“œ í•´ì‹œë¡œ ë™ì¼ ì½”ë“œ ë¸”ë¡ ê°ì§€
- **ì˜ë¯¸ì  ëì **: í•¨ìˆ˜/í´ë˜ìŠ¤ ëì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­

### 1. í—¬ìŠ¤ ì²´í¬

```python
@app.get("/health")
async def health_check():
    """ê¸°ë³¸ í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/health/detailed")
async def detailed_health_check():
    """ìƒì„¸ í—¬ìŠ¤ ì²´í¬"""
    checks = {
        "redis": await check_redis_connection(),
        "embedding_service": await check_embedding_service(),
        "llm_service": await check_llm_service()
    }
    return {"status": "healthy" if all(checks.values()) else "unhealthy", "checks": checks}
```

### 2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
@dataclass
class SimpleMetrics:
    source_type: str
    source_key: str
    start_time: float
    documents_processed: int = 0
    errors_count: int = 0

    def success_rate(self) -> float:
        total = self.documents_processed + self.errors_count
        return self.documents_processed / total if total > 0 else 0.0

class MetricsCollector:
    def get_summary(self) -> dict:
        return {
            "total_sources": len(self.metrics),
            "sources": {k: {
                "duration": v.duration(),
                "documents": v.documents_processed,
                "success_rate": v.success_rate()
            } for k, v in self.metrics.items()}
        }
```

## ğŸ” ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. ì¸ì¦ ì •ë³´ ê´€ë¦¬

- ëª¨ë“  API í‚¤/í† í°ì€ **í™˜ê²½ë³€ìˆ˜**ë¡œ ê´€ë¦¬
- `.env` íŒŒì¼ ë° ë¯¼ê° ì •ë³´ëŠ” **gitì—ì„œ ì œì™¸**
- ë¡œê·¸ì— ì¸ì¦ ì •ë³´ **ë…¸ì¶œ ë°©ì§€**

### 2. API ì—°ê²° ê²€ì¦

```python
class ConnectionValidator:
    async def validate_all_connections(self) -> dict:
        """ì‹œì‘ ì‹œ ëª¨ë“  ì™¸ë¶€ ì„œë¹„ìŠ¤ ì—°ê²° ê²€ì¦"""
        results = {}

        if self.settings.slack_token:
            results["slack"] = await self._validate_slack()
        if self.settings.github_app_id:
            results["github"] = await self._validate_github()

        return results

    async def _validate_slack(self) -> dict:
        """Slack ì—°ê²° ê²€ì¦ (auth.test í˜¸ì¶œ)"""
        try:
            client = AsyncWebClient(token=self.settings.slack_token)
            response = await client.auth_test()
            return {"status": "success", "user_id": response["user_id"]}
        except Exception as e:
            return {"status": "failed", "error": str(e)}
```

## ğŸ”„ í™•ì¥ ë°©ë²•

### ìƒˆë¡œìš´ Executor ì¶”ê°€

1. **ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±**

```
loaders/new_source/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ sources.yaml
â”œâ”€â”€ executor.py     # ì‹¤í–‰ê¸°
â”œâ”€â”€ client.py
â””â”€â”€ entities.py     # ë°ì´í„° ëª¨ë¸
```

2. **BaseExecutor êµ¬í˜„**

```python
from core.base import BaseExecutor, DateRange, Document

class NewSourceExecutor(BaseExecutor):
    def __init__(self, config: dict):
        super().__init__(config)
        self.client = NewSourceClient(config)

    async def fetch(self, date_range: DateRange) -> AsyncGenerator[Document, None]:
        """ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë¬¸ì„œ ë¡œë“œ"""

        # ì¦ë¶„ ì—…ë°ì´íŠ¸
        params = self._build_query_params(date_range)

        # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
        async for item in self._paginate_fetch(params):
            document = self._item_to_document(item)
            yield document

    def _build_query_params(self, date_range: DateRange) -> dict:
        """DateRangeë¥¼ API íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜"""
        params = {}
        if date_range.start:
            params['modified_since'] = date_range.start.isoformat()
        if date_range.end:
            params['modified_until'] = date_range.end.isoformat()
        return params
```

3. **LoaderExecutorì— ë“±ë¡**

```python
# executor.py
self.executors = {
    "slack": SlackExecutor(settings.slack),
    "github": GitHubExecutor(settings.github),
    "confluence": ConfluenceExecutor(settings.confluence),
    "new_source": NewSourceExecutor(settings.new_source)  # ì¶”ê°€
}
```

## ğŸ¯ ê°œì„  ìš”ì•½

### âœ… ì ìš©ëœ í•µì‹¬ íŒ¨í„´ë“¤

1. **BaseExecutor íŒ¨í„´**: ì¼ê´€ëœ `fetch()` ì¸í„°í˜ì´ìŠ¤
2. **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: `AsyncGenerator[Document, None]` ë°˜í™˜
3. **ê²¬ê³ í•œ ì¬ì‹œë„**: ì§€ìˆ˜ ë°±ì˜¤í”„ + HTTP ìƒíƒœì½”ë“œë³„ ì²˜ë¦¬
4. **DateRange ê¸°ë°˜**: ì¦ë¶„ ì—…ë°ì´íŠ¸ ìµœì í™”
5. **í˜ì´ì§€ë„¤ì´ì…˜**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëŒ€ìš©ëŸ‰ ì²˜ë¦¬
6. **ë°°ì¹˜ ì²˜ë¦¬**: ìŠ¤íŠ¸ë¦¬ë°ê³¼ ê²°í•©í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬

### ğŸ¯ ì‹¬í”Œí•¨ ìœ ì§€

- **ë³µì¡í•œ Circuit Breaker ì œê±°** â†’ ì‹¬í”Œí•œ ì¬ì‹œë„ë§Œ
- **ìš”ì•½ ê¸°ëŠ¥ì€ ì˜µì…˜** â†’ ê¸°ë³¸ ê¸°ëŠ¥ì— ì§‘ì¤‘
- **ìµœì†Œí•œì˜ ì¸í„°í˜ì´ìŠ¤** â†’ BaseExecutor í•˜ë‚˜ë¡œ í†µì¼
- **ì„¤ì • ê¸°ë°˜ í™•ì¥** â†’ ì½”ë“œ ë³€ê²½ ìµœì†Œí™”

ì´ ì•„í‚¤í…ì²˜ëŠ” **ê²¬ê³ í•¨ê³¼ ì‹¬í”Œí•¨**ì„ ê²°í•©í•œ êµ¬ì¡°ë¡œ, í™•ì¥ì„±ê³¼ ìœ ì§€ë³´ìˆ˜ì„±ì„ ëª¨ë‘ í™•ë³´í–ˆìŠµë‹ˆë‹¤.
